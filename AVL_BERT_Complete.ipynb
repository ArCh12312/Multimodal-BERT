{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9a3f6ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_loader import IEMOCAPLoader\n",
    "\n",
    "TRANSCRIPT_DIR = \"C:\\\\Users\\\\aryan\\\\Documents\\\\Study\\\\Research\\\\IEMOCAP_full_release\\\\Session1\\\\dialog\\\\transcriptions\"\n",
    "AUDIO_DIR = \"C:\\\\Users\\\\aryan\\\\Documents\\\\Study\\\\Research\\\\Audio_Output\"\n",
    "VIDEO_DIR = \"C:\\\\Users\\\\aryan\\\\Documents\\\\Study\\\\Research\\\\Video_Output\"\n",
    "LABEL_FILE = \"C:\\\\Users\\\\aryan\\\\Documents\\\\Study\\\\Research\\\\scene_emotions.csv\"\n",
    "\n",
    "loader = IEMOCAPLoader(TRANSCRIPT_DIR, AUDIO_DIR, VIDEO_DIR, LABEL_FILE)\n",
    "dataset = loader.get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "77475a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 151 scenes.\n",
      "Sample entry: {'scene_id': 'Ses01F_impro01', 'transcript': \"Excuse me.\\nDo you have your forms?\\nYeah.\\nLet me see them.\\nIs there a problem?\\nWho told you to get in this line?\\nYou did.\\nYou were standing at the beginning and you directed me.\\nOkay. But I didn't tell you to get in this line if you are filling out this particular form.\\nWell what's the problem?  Let me change it.\\nThis form is a Z.X.four.\\nYou can't--  This is not the line for Z.X.four.  If you're going to fill out the Z.X.four, you need to have a different form of ID.\\nWhat?  I'm getting an ID.  This is why I'm here.  My wallet was stolen.\\nNo. I need another set of ID to prove this is actually you.\\nHow am I supposed to get an ID without an ID?  How does a person get an ID in the first place?\\nI don't know.  But I need an ID to pass this form along.  I can't just send it along without an ID.\\nI'm here to get an ID.\\nNo.  I need another ID, a separate one.\\nLike what?  Like a birth certificate?\\nA birth certificate, a passport...a student ID; didn't you go to school?  Anything?\\nWho the hell has a birth certificate?\\nYes but my wallet was stolen, I don't have anything.  I don't have any credit cards, I don't have my ID.  Don't you have things on file here?\\nYeah.  We keep it on file, but we need an ID to access that file.\\nThat's out of control.\\nI don't understand why this is so complicated for people when they get here.  It's just a simple form.  I just need an ID.\\nHow long have you been working here?\\nActually too long.\\nClearly.  You know, do you have like a supervisor or something?\\nYeah.  Do you want to see my supervisor?  Huh? Yeah.  Do you want to see my supervisor?  Fine.  I'll be right back.\\nThat would - I would appreciate that.  Yeah.\", 'audio_path': 'C:\\\\Users\\\\aryan\\\\Documents\\\\Study\\\\Research\\\\Audio_Output\\\\Ses01F_impro01.npy', 'video_path': 'C:\\\\Users\\\\aryan\\\\Documents\\\\Study\\\\Research\\\\Video_Output\\\\Ses01F_impro01.npy', 'emotion_counts': {'Anger': 0.1111111111111111, 'Disgust': 0.0333333333333333, 'Excited': 0.0, 'Fear': 0.0, 'Frustration': 0.6222222222222222, 'Happiness': 0.0, 'Neutral state': 0.2, 'Other': 0.0111111111111111, 'Sadness': 0.0111111111111111, 'Surprise': 0.0111111111111111}}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loaded {len(dataset)} scenes.\")\n",
    "print(\"Sample entry:\", dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0d1d8b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_embedding import BERTFeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "93870573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted features shape: torch.Size([151, 125, 768])\n"
     ]
    }
   ],
   "source": [
    "# Extract just the transcript texts\n",
    "texts = [item[\"transcript\"] for item in dataset]\n",
    "\n",
    "extractor = BERTFeatureExtractor()\n",
    "text_embeddings = extractor.extract_features(texts)\n",
    "\n",
    "print(\"Extracted features shape:\", text_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "dc1e074e",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_embedding_paths = [item[\"video_path\"] for item in dataset]\n",
    "audio_embedding_paths = [item[\"audio_path\"] for item in dataset]\n",
    "video_embeddings = []\n",
    "audio_embeddings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9120f44c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded video embeddings\n",
      "\n",
      "Loaded audio embeddings\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "for path in video_embedding_paths:\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            embedding = np.load(path)\n",
    "            video_embeddings.append(embedding)\n",
    "            # print(f\"Loaded video embedding from: {path} (Shape: {embedding.shape})\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading video embedding from {path}: {e}\")\n",
    "    else:\n",
    "        print(f\"Video embedding file not found: {path}\")\n",
    "print(\"\\nLoaded video embeddings\")\n",
    "\n",
    "for path in audio_embedding_paths:\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            embedding = np.load(path)\n",
    "            audio_embeddings.append(embedding)\n",
    "            # print(f\"Loaded audio embedding from: {path} (Shape: {embedding.shape})\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading audio embedding from {path}: {e}\")\n",
    "    else:\n",
    "        print(f\"Audio embedding file not found: {path}\")\n",
    "print(\"\\nLoaded audio embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "71da22a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully loaded 151 video embeddings.\n",
      "Successfully loaded 151 audio embeddings.\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nSuccessfully loaded {len(video_embeddings)} video embeddings.\")\n",
    "print(f\"Successfully loaded {len(audio_embeddings)} audio embeddings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "180b43ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_embeddings = torch.from_numpy(np.stack(video_embeddings))\n",
    "audio_embeddings = torch.from_numpy(np.stack(audio_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "84a9b124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([151, 256, 768])\n",
      "torch.Size([151, 128, 768])\n"
     ]
    }
   ],
   "source": [
    "print(video_embeddings.shape)\n",
    "print(audio_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "688a0486",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e57c32d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "83a4a161",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_embedding = model.embeddings.word_embeddings(torch.tensor([tokenizer.cls_token_id]))\n",
    "sep_embedding = model.embeddings.word_embeddings(torch.tensor([tokenizer.sep_token_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6494befb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the CLS embedding: torch.Size([1, 768])\n",
      "Shape of the SEP embedding: torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of the CLS embedding:\", cls_embedding.shape)\n",
    "print(\"Shape of the SEP embedding:\", sep_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "312b0025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_modalities(cls_token, sep_token, video_tensor, audio_tensor, text_tensor):\n",
    "    fused = []\n",
    "    for v, a, t in zip(video_tensor, audio_tensor, text_tensor):\n",
    "        segments = [\n",
    "            cls_token,  # (1, 768)\n",
    "            v,          # (v_len, 768)\n",
    "            sep_token,  # (1, 768)\n",
    "            a,          # (a_len, 768)\n",
    "            sep_token,  # (1, 768)\n",
    "            t          # (t_len, 768)\n",
    "        ]\n",
    "        fused.append(torch.cat(segments, dim=0))  # (total_len, 768)\n",
    "\n",
    "    return fused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b0ccb77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fused_embeddings = concatenate_modalities(cls_embedding, sep_embedding, video_embeddings, audio_embeddings, text_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "00cc00ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 768])\n"
     ]
    }
   ],
   "source": [
    "print(fused_embeddings[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "30ff736e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModel\n",
    "from torch.optim import AdamW\n",
    "from transformers import BigBirdModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8d19347c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Dataset class\n",
    "# ----------------------------\n",
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, embeddings, labels):\n",
    "        \"\"\"\n",
    "        embeddings: torch.Tensor [num_samples, 512, 768]\n",
    "        labels: torch.Tensor [num_samples]\n",
    "        \"\"\"\n",
    "        self.embeddings = embeddings\n",
    "        self.labels = labels\n",
    "        print(f\"Dataset initialized with {len(self.embeddings)} samples, {len(self.labels)} labels\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ed54ce75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Model class\n",
    "# ----------------------------\n",
    "class BertEmbeddingClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(768),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(768, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs_embeds):\n",
    "        \"\"\"\n",
    "        inputs_embeds: [batch_size, 512, 768]\n",
    "        \"\"\"\n",
    "        outputs = self.bert(inputs_embeds=inputs_embeds)\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :]  # CLS token embedding\n",
    "        logits = self.classifier(cls_embedding)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "38666560",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigBirdEmbeddingClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.bigbird = BigBirdModel.from_pretrained(\"google/bigbird-roberta-base\")\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(768),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(768, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs_embeds):\n",
    "        \"\"\"\n",
    "        inputs_embeds: [batch_size, seq_len, 768]\n",
    "        \"\"\"\n",
    "        outputs = self.bigbird(inputs_embeds=inputs_embeds)\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "        logits = self.classifier(cls_embedding)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8484e5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_embeds, batch_labels in dataloader:\n",
    "        batch_embeds = batch_embeds.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(batch_embeds)\n",
    "\n",
    "        log_probs = F.log_softmax(logits, dim=1)  # convert logits to log-probabilities\n",
    "        loss = criterion(log_probs, batch_labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch_embeds, batch_labels in dataloader:\n",
    "            batch_embeds = batch_embeds.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "\n",
    "            logits = model(batch_embeds)\n",
    "            preds = torch.argmax(logits, dim=1)  # predicted classes (hard)\n",
    "\n",
    "            true_labels = torch.argmax(batch_labels, dim=1)  # convert soft labels to hard\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(true_labels.cpu().numpy())\n",
    "\n",
    "    return all_labels, all_preds\n",
    "\n",
    "def evaluatedist(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_kl = 0.0\n",
    "    total_mse = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_embeds, batch_labels in dataloader:\n",
    "            batch_embeds = batch_embeds.to(device)\n",
    "            batch_labels = batch_labels.to(device)  # [batch_size, num_classes]\n",
    "\n",
    "            logits = model(batch_embeds)  # [batch_size, num_classes]\n",
    "            pred_log_probs = F.log_softmax(logits, dim=1)\n",
    "            pred_probs = torch.softmax(logits, dim=1)\n",
    "\n",
    "            # KL divergence (predicted log probs vs ground-truth probs)\n",
    "            kl_loss = F.kl_div(pred_log_probs, batch_labels, reduction='batchmean')\n",
    "\n",
    "            # MSE between predicted and ground-truth probability vectors\n",
    "            mse_loss = F.mse_loss(pred_probs, batch_labels)\n",
    "\n",
    "            total_kl += kl_loss.item()\n",
    "            total_mse += mse_loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "    avg_kl = total_kl / num_batches\n",
    "    avg_mse = total_mse / num_batches\n",
    "\n",
    "    return avg_kl, avg_mse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ce140bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels tensor shape: torch.Size([151, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Extract all emotions from one sample (assuming all have same keys)\n",
    "all_emotions = list(dataset[0]['emotion_counts'].keys())\n",
    "\n",
    "# Build a tensor of soft labels (probabilities) from dataset\n",
    "labels_list = []\n",
    "for sample in dataset:\n",
    "    probs = [sample['emotion_counts'][emo] for emo in all_emotions]\n",
    "    labels_list.append(probs)\n",
    "\n",
    "labels_tensor = torch.tensor(labels_list, dtype=torch.float)\n",
    "print(\"Labels tensor shape:\", labels_tensor.shape)\n",
    "\n",
    "embeddings_tensor = torch.stack(fused_embeddings).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ebd89617",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4adfe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "--- Fold 1 ---\n",
      "Dataset initialized with 120 samples, 120 labels\n",
      "Dataset initialized with 31 samples, 31 labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aryan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_split.py:700: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to list.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[92], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m all_labels, all_preds \u001b[38;5;241m=\u001b[39m evaluate(model, test_loader, device)\n\u001b[0;32m     42\u001b[0m avg_kl, avg_mse \u001b[38;5;241m=\u001b[39m evaluate(model, test_loader, device)\n\u001b[1;32m---> 43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKL Divergence: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_kl\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, MSE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_mse\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# F1 Score\u001b[39;00m\n\u001b[0;32m     46\u001b[0m f1 \u001b[38;5;241m=\u001b[39m f1_score(all_labels, all_preds, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported format string passed to list.__format__"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Main\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Get number of classes and samples\n",
    "    num_classes = labels_tensor.shape[1]\n",
    "    num_samples = labels_tensor.shape[0]\n",
    "\n",
    "\n",
    "    hard_labels_for_split = torch.argmax(labels_tensor, dim=1).numpy()\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    all_accuracies = []\n",
    "    all_f1_scores = []\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(skf.split(embeddings_tensor, hard_labels_for_split)):\n",
    "        print(f\"\\n--- Fold {fold+1} ---\")\n",
    "\n",
    "        train_embeds = embeddings_tensor[train_idx]\n",
    "        train_labels = labels_tensor[train_idx]\n",
    "        test_embeds = embeddings_tensor[test_idx]\n",
    "        test_labels = labels_tensor[test_idx]\n",
    "\n",
    "        train_dataset = EmbeddingDataset(train_embeds, train_labels)\n",
    "        test_dataset = EmbeddingDataset(test_embeds, test_labels)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "        model = BertEmbeddingClassifier(num_classes=num_classes).to(device)\n",
    "        optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "        criterion = nn.KLDivLoss(reduction=\"batchmean\")  # for soft labels\n",
    "\n",
    "        for epoch in range(20):  # You can adjust this\n",
    "            train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "\n",
    "        all_labels, all_preds = evaluate(model, test_loader, device)\n",
    "\n",
    "        avg_kl, avg_mse = evaluatedist(model, test_loader, device)\n",
    "        print(f\"KL Divergence: {avg_kl:.4f}, MSE: {avg_mse:.4f}\")\n",
    "        \n",
    "        # F1 Score\n",
    "        f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "        print(f\"Fold {fold+1} Weighted F1-score: {f1:.4f}\")\n",
    "        \n",
    "        # Accuracy\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        print(f\"Fold {fold+1} Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "        # Store metrics\n",
    "        all_accuracies.append(accuracy)\n",
    "        all_f1_scores.append(f1) \n",
    "\n",
    "        # print(classification_report(all_labels, all_preds, target_names=label_encoder.classes_))\n",
    "\n",
    "    print(f\"\\nAverage Weighted F1-score across folds: {np.mean(all_accuracies):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
